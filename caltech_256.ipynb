{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "root_dir = '../input/caltech256/256_ObjectCategories'\n",
    "# get all the folder paths\n",
    "all_paths = os.listdir(root_dir)\n",
    "\n",
    "# create a DataFrame\n",
    "data = pd.DataFrame()\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "counter = 0\n",
    "for folder_path in tqdm(all_paths, total=len(all_paths)):\n",
    "    # get all the image names in the particular folder\n",
    "    image_paths = os.listdir(f\"{root_dir}/{folder_path}\")\n",
    "    # get the folder as label\n",
    "    label = folder_path.split('.')[-1]\n",
    "    \n",
    "    if label == 'clutter':\n",
    "        continue\n",
    "\n",
    "    # save image paths in the DataFrame\n",
    "    for image_path in image_paths:\n",
    "        if image_path.split('.')[-1] == 'jpg':\n",
    "            data.loc[counter, 'image_path'] = f\"{root_dir}/{folder_path}/{image_path}\"\n",
    "            labels.append(label)\n",
    "            counter += 1\n",
    "\n",
    "labels = np.array(labels)\n",
    "# one-hot encode the labels\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "# add the image labels to the dataframe\n",
    "for i in range(len(labels)):\n",
    "    index = np.argmax(labels[i])\n",
    "    data.loc[i, 'target'] = int(index)\n",
    "    \n",
    "# shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of labels or classes: {len(lb.classes_)}\")\n",
    "print(f\"The first one hot encoded labels: {labels[0]}\")\n",
    "print(f\"Mapping the first one hot encoded label to its category: {lb.classes_[0]}\")\n",
    "print(f\"Total instances: {len(data)}\")\n",
    " \n",
    "# save as CSV file\n",
    "data.to_csv('data.csv', index=False)\n",
    " \n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset.py\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels=None, tfms=None):\n",
    "        self.X = images\n",
    "        self.y = labels\n",
    "\n",
    "        # apply augmentations\n",
    "        if tfms == 0: # if validating\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "            ])\n",
    "        else: # if training\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(224, 224, always_apply=True),\n",
    "                albumentations.HorizontalFlip(p=0.5),\n",
    "                albumentations.ShiftScaleRotate(\n",
    "                    shift_limit=0.3,\n",
    "                    scale_limit=0.3,\n",
    "                    rotate_limit=15,\n",
    "                    p=0.5\n",
    "                ),\n",
    "            ])\n",
    "         \n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.X[i])\n",
    "        image = image.convert('RGB')\n",
    "        image = self.aug(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        label = self.y[i]\n",
    "        return {\n",
    "            'image': torch.tensor(image, dtype=torch.float), \n",
    "            'target': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "import pretrainedmodels\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, pretrained, requires_grad):\n",
    "        super(ResNet50, self).__init__()\n",
    "        if pretrained is True:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained='imagenet')\n",
    "        else:\n",
    "            self.model = pretrainedmodels.__dict__['resnet50'](pretrained=None)\n",
    "            \n",
    "        if requires_grad == True:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif requires_grad == False:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.l0 = nn.Linear(2048, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = self.model.features(x)\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        l0 = self.l0(x)\n",
    "        return l0\n",
    "\n",
    "model = ResNet50(pretrained=True, requires_grad=False)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import model\n",
    "from dataset import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# get the dataset ready\n",
    "df = pd.read_csv('data.csv')\n",
    "X = df.image_path.values # image paths\n",
    "y = df.target.values # targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = ImageDataset(X, y, tfms=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[167., 168., 170.,  ..., 181., 181., 182.],\n",
       "          [168., 169., 169.,  ..., 177., 178., 178.],\n",
       "          [169., 169., 169.,  ..., 179., 178., 178.],\n",
       "          ...,\n",
       "          [165., 165., 166.,  ..., 177., 177., 177.],\n",
       "          [167., 166., 166.,  ..., 177., 177., 176.],\n",
       "          [168., 167., 166.,  ..., 177., 177., 176.]],\n",
       " \n",
       "         [[176., 177., 179.,  ..., 190., 191., 191.],\n",
       "          [177., 178., 178.,  ..., 186., 187., 187.],\n",
       "          [177., 178., 178.,  ..., 188., 188., 187.],\n",
       "          ...,\n",
       "          [174., 174., 175.,  ..., 187., 187., 187.],\n",
       "          [176., 175., 175.,  ..., 186., 186., 185.],\n",
       "          [177., 176., 175.,  ..., 186., 186., 185.]],\n",
       " \n",
       "         [[173., 174., 176.,  ..., 187., 187., 188.],\n",
       "          [174., 175., 175.,  ..., 184., 184., 184.],\n",
       "          [175., 175., 175.,  ..., 185., 185., 184.],\n",
       "          ...,\n",
       "          [171., 171., 172.,  ..., 184., 184., 184.],\n",
       "          [173., 172., 172.,  ..., 183., 183., 182.],\n",
       "          [174., 173., 172.,  ..., 183., 183., 182.]]]),\n",
       " 'target': tensor(245)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X, batch_y = next(image_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model(image_data[0]['image'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4ebfc88ccd97cb231efe00c7198020b2ae0235bcff1ce31852dbbe06876d933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
